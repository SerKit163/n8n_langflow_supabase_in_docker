services:
  n8n:
    image: n8nio/n8n:latest
    container_name: n8n
    environment:
      - N8N_HOST=0.0.0.0
      - N8N_PORT=5678
      - N8N_PROTOCOL=${N8N_PROTOCOL:-http}
      - WEBHOOK_URL=${WEBHOOK_URL:-http://localhost:5678/}
      - VIRTUAL_HOST=${VIRTUAL_HOST_N8N:-}
      - LETSENCRYPT_HOST=${LETSENCRYPT_HOST_N8N:-}
      - LETSENCRYPT_EMAIL=${LETSENCRYPT_EMAIL:-}
    ports:
      - "${N8N_PORT}:5678"
    deploy:
      resources:
        limits:
          memory: ${N8N_MEMORY_LIMIT}
          cpus: '${N8N_CPU_LIMIT}'
        reservations:
          memory: ${N8N_MEMORY_LIMIT}
          cpus: '${N8N_CPU_LIMIT}'
    volumes:
      - n8n_data:/home/node/.n8n
    networks:
      - proxy
    restart: unless-stopped
    entrypoint: |
      sh -c "
        chown -R node:node /home/node/.n8n || true
        chmod -R 755 /home/node/.n8n || true
        exec tini -- /docker-entrypoint.sh
      "
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:5678/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  langflow:
    image: langflowai/langflow:latest
    container_name: langflow
    environment:
      - LANGFLOW_API_KEY=${LANGFLOW_API_KEY:-}
      - LANGFLOW_HOST=0.0.0.0
      - LANGFLOW_PORT=7860
      - LANGFLOW_CACHE_TYPE=memory
      - LANGFLOW_LANGCHAIN_CACHE=memory
      - VIRTUAL_HOST=${VIRTUAL_HOST_LANGFLOW:-}
      - LETSENCRYPT_HOST=${LETSENCRYPT_HOST_LANGFLOW:-}
      - LETSENCRYPT_EMAIL=${LETSENCRYPT_EMAIL:-}
    ports:
      - "${LANGFLOW_PORT}:7860"
    deploy:
      resources:
        limits:
          memory: ${LANGFLOW_MEMORY_LIMIT}
          cpus: '${LANGFLOW_CPU_LIMIT}'
        reservations:
          memory: ${LANGFLOW_MEMORY_LIMIT}
          cpus: '${LANGFLOW_CPU_LIMIT}'
    volumes:
      - langflow_data:/app/data
    networks:
      - proxy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:7860/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  supabase-db:
    image: ghcr.io/supabase/postgres:15.1.0.119
    container_name: supabase-db
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=postgres
      - POSTGRES_HOST=/var/run/postgresql
      - PGDATA=/var/lib/postgresql/data/pgdata
    ports:
      - "${SUPABASE_PORT}:5432"
    deploy:
      resources:
        limits:
          memory: ${SUPABASE_MEMORY_LIMIT}
          cpus: '${SUPABASE_CPU_LIMIT}'
        reservations:
          memory: ${SUPABASE_MEMORY_LIMIT}
          cpus: '${SUPABASE_CPU_LIMIT}'
    volumes:
      - supabase_data:/var/lib/postgresql/data
    networks:
      - proxy
    restart: unless-stopped
    entrypoint: |
      sh -c "
        if [ ! -d /var/lib/postgresql/data/pgdata ]; then
          mkdir -p /var/lib/postgresql/data/pgdata
        fi
        chown -R postgres:postgres /var/lib/postgresql/data
        chmod 700 /var/lib/postgresql/data/pgdata
        exec docker-entrypoint.sh postgres -c listen_addresses='*'
      "
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  supabase-studio:
    image: ghcr.io/supabase/studio:20240513-d025e0f
    container_name: supabase-studio
    environment:
      - SUPABASE_URL=http://supabase-rest:3000
      - SUPABASE_PUBLIC_URL=${SUPABASE_PUBLIC_URL:-http://localhost:3000}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - STUDIO_PGUSER=postgres
      - STUDIO_PGPASSWORD=${POSTGRES_PASSWORD}
      - DEFAULT_ORGANIZATION_NAME=Default Organization
      - DEFAULT_PROJECT_NAME=Default Project
      - SUPABASE_DB_URL=postgresql://postgres:${POSTGRES_PASSWORD}@supabase-db:5432/postgres
      - SUPABASE_DB_PASSWORD=${POSTGRES_PASSWORD}
      - VIRTUAL_HOST=${VIRTUAL_HOST_SUPABASE:-}
      - LETSENCRYPT_HOST=${LETSENCRYPT_HOST_SUPABASE:-}
      - LETSENCRYPT_EMAIL=${LETSENCRYPT_EMAIL:-}
    ports:
      - "127.0.0.1:${SUPABASE_KB_PORT:-3000}:3000"
    depends_on:
      - supabase-rest
      - supabase-db
    networks:
      - proxy
    restart: unless-stopped

  supabase-auth:
    image: ghcr.io/supabase/gotrue:v2.162.0
    container_name: supabase-auth
    depends_on:
      - supabase-db
    environment:
      - API_EXTERNAL_URL=${SUPABASE_PUBLIC_URL:-http://localhost:8000}
      - DATABASE_URL=postgresql://postgres:${POSTGRES_PASSWORD}@supabase-db:5432/postgres
      - JWT_SECRET=${JWT_SECRET}
      - ANON_KEY=${ANON_KEY}
      - SERVICE_ROLE_KEY=${SERVICE_ROLE_KEY}
      - LOG_LEVEL=info
    networks:
      - proxy
    restart: unless-stopped

  supabase-rest:
    image: ghcr.io/supabase/postgrest:v12.2.0
    container_name: supabase-rest
    depends_on:
      - supabase-db
    environment:
      - PGRST_DB_URI=postgresql://postgres:${POSTGRES_PASSWORD}@supabase-db:5432/postgres
      - PGRST_DB_SCHEMAS=public,storage,graphql_public
      - PGRST_DB_ANON_ROLE=anon
      - PGRST_SERVER_HOST=0.0.0.0
      - PGRST_SERVER_PORT=3000
    networks:
      - proxy
    restart: unless-stopped

  ollama:
    image: ${OLLAMA_IMAGE:-ollama/ollama:latest}
    container_name: ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - VIRTUAL_HOST=${VIRTUAL_HOST_OLLAMA:-}
      - LETSENCRYPT_HOST=${LETSENCRYPT_HOST_OLLAMA:-}
      - LETSENCRYPT_EMAIL=${LETSENCRYPT_EMAIL:-}
    ports:
      - "${OLLAMA_PORT}:11434"
    deploy:
      resources:
        limits:
          memory: ${OLLAMA_MEMORY_LIMIT}
          cpus: '${OLLAMA_CPU_LIMIT}'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - proxy
    restart: unless-stopped
    entrypoint: |
      sh -c "
        mkdir -p /root/.ollama
        chmod -R 755 /root/.ollama || true
        exec /bin/ollama serve
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  nginx-proxy:
    image: nginxproxy/nginx-proxy:latest
    container_name: nginx-proxy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - certs:/etc/nginx/certs:ro
      - vhost.d:/etc/nginx/vhost.d:ro
      - html:/usr/share/nginx/html:ro
    networks:
      - proxy
    restart: unless-stopped
    labels:
      - "com.github.nginx-proxy.nginx-proxy"

  nginx-proxy-letsencrypt:
    image: nginxproxy/acme-companion:latest
    container_name: nginx-proxy-letsencrypt
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - certs:/etc/nginx/certs:rw
      - vhost.d:/etc/nginx/vhost.d:rw
      - html:/usr/share/nginx/html:rw
      - acme:/etc/acme.sh
    environment:
      - DEFAULT_EMAIL=${LETSENCRYPT_EMAIL:-}
      - NGINX_PROXY_CONTAINER=nginx-proxy
    networks:
      - proxy
    depends_on:
      - nginx-proxy
    restart: unless-stopped

volumes:
  n8n_data:
    driver: local
  langflow_data:
    driver: local
  supabase_data:
    driver: local
  ollama_data:
    driver: local
  certs:
    driver: local
  vhost.d:
    driver: local
  html:
    driver: local
  acme:
    driver: local

networks:
  proxy:
    driver: bridge
